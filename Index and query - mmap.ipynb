{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T15:38:38.556555",
     "start_time": "2016-11-02T15:38:38.552552"
    }
   },
   "source": [
    "# Query lexer. Very beta\n",
    "\n",
    "It works rather well, almost surprisingly. Only with and or or, and parenthesis, I did not really test the quotes and not operator (spoiler alert : it means they don't work. at all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T15:38:46.071938",
     "start_time": "2016-11-02T15:38:45.935099"
    },
    "code_folding": [
     47
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Search query parser\n",
    "\n",
    "version 2006-03-09\n",
    "\n",
    "This search query parser uses the excellent Pyparsing module \n",
    "(http://pyparsing.sourceforge.net/) to parse search queries by users.\n",
    "-------------------------------------------------------------------------------\n",
    "Copyright (c) 2006, Estrate, the Netherlands\n",
    "All rights reserved.\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without modification,\n",
    "are permitted provided that the following conditions are met:\n",
    "\n",
    "* Redistributions of source code must retain the above copyright notice, this\n",
    "  list of conditions and the following disclaimer.\n",
    "* Redistributions in binary form must reproduce the above copyright notice,\n",
    "  this list of conditions and the following disclaimer in the documentation \n",
    "  and/or other materials provided with the distribution.\n",
    "* Neither the name of Estrate nor the names of its contributors may be used\n",
    "  to endorse or promote products derived from this software without specific\n",
    "  prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
    "ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
    "WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n",
    "ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n",
    "(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; \n",
    "LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON \n",
    "ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT \n",
    "(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n",
    "SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "CONTRIBUTORS:\n",
    "- Steven Mooij\n",
    "- Rudolph Froger\n",
    "- Paul McGuire\n",
    "\n",
    "\n",
    "~~ End of notice\n",
    "\n",
    "I did not alter the structure of this thing a lot. Mostly modified it to work in my implementation of the index,\n",
    "which uses tuples docid + score instead of single ids. Thus some things might not work yet.\n",
    "Also hurray for global variables and definitions, because I didn't want to write more classes.\n",
    "Wildcard and not search are broken. I didn't test them anyway.\n",
    "\"\"\"\n",
    "from pyparsing import Word, alphanums, Keyword, Group, Combine, Forward, Suppress, Optional, OneOrMore, oneOf\n",
    "\n",
    "class SearchQueryParser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._methods = {\n",
    "            'and': self.evaluateAnd,\n",
    "            'or': self.evaluateOr,\n",
    "            'not': self.evaluateNot,\n",
    "            'parenthesis': self.evaluateParenthesis,\n",
    "            'quotes': self.evaluateQuotes,\n",
    "            'word': self.evaluateWord,\n",
    "            'wordwildcard': self.evaluateWordWildcard,\n",
    "        }\n",
    "        self._parser = self.parser()\n",
    "    \n",
    "    def parser(self):\n",
    "        operatorOr = Forward()\n",
    "        \n",
    "        operatorWord = Group(Combine(Word(alphanums) + Suppress('*'))).setResultsName('wordwildcard') | \\\n",
    "                            Group(Word(alphanums)).setResultsName('word')\n",
    "        \n",
    "        operatorQuotesContent = Forward()\n",
    "        operatorQuotesContent << (\n",
    "            (operatorWord + operatorQuotesContent) | operatorWord\n",
    "        )\n",
    "        \n",
    "        operatorQuotes = Group(\n",
    "            Suppress('\"') + operatorQuotesContent + Suppress('\"')\n",
    "        ).setResultsName(\"quotes\") | operatorWord\n",
    "        \n",
    "        operatorParenthesis = Group(\n",
    "            (Suppress(\"(\") + operatorOr + Suppress(\")\"))\n",
    "        ).setResultsName(\"parenthesis\") | operatorQuotes\n",
    "\n",
    "        operatorNot = Forward()\n",
    "        operatorNot << (Group(\n",
    "            Suppress(Keyword(\"not\", caseless=True)) + operatorNot\n",
    "        ).setResultsName(\"not\") | operatorParenthesis)\n",
    "\n",
    "        operatorAnd = Forward()\n",
    "        operatorAnd << (Group(\n",
    "            operatorNot + Suppress(Keyword(\"and\", caseless=True)) + operatorAnd\n",
    "        ).setResultsName(\"and\") | Group(\n",
    "            operatorNot + OneOrMore(~oneOf(\"and or\") + operatorAnd)\n",
    "        ).setResultsName(\"and\") | operatorNot)\n",
    "        \n",
    "        operatorOr << (Group(\n",
    "            operatorAnd + Suppress(Keyword(\"or\", caseless=True)) + operatorOr\n",
    "        ).setResultsName(\"or\") | operatorAnd)\n",
    "\n",
    "        return operatorOr.parseString\n",
    "\n",
    "    def evaluateAnd(self, argument):\n",
    "        left = self.evaluate(argument[0])\n",
    "        right = self.evaluate(argument[1])\n",
    "        \n",
    "        found = set(left.keys()).intersection(set(right.keys()))\n",
    "        scores = {}\n",
    "        for key in found:\n",
    "            scores[key] = left[key] + right[key]\n",
    "        return scores\n",
    "\n",
    "    def evaluateOr(self, argument):\n",
    "        scores = {}\n",
    "        for i in range(0,2):\n",
    "            localresults = self.evaluate(argument[i])\n",
    "            for key in localresults.keys():\n",
    "                    scores[key] = scores.setdefault(key, 0) + localresults[key]\n",
    "        return scores\n",
    "\n",
    "    def evaluateNot(self, argument):\n",
    "        return self.GetNot(self.evaluate(argument[0]))\n",
    "\n",
    "    def evaluateParenthesis(self, argument):\n",
    "        return self.evaluate(argument[0])\n",
    "\n",
    "    def evaluateQuotes(self, argument):\n",
    "        r = Set()\n",
    "        search_terms = []\n",
    "        for item in argument:\n",
    "            search_terms.append(item[0])\n",
    "            if len(r) == 0:\n",
    "                r = self.evaluate(item)\n",
    "            else:\n",
    "                r = r.intersection(self.evaluate(item))\n",
    "        return self.GetQuotes(' '.join(search_terms), r)\n",
    "\n",
    "    def evaluateWord(self, argument):\n",
    "        return self.GetWord(argument[0])\n",
    "\n",
    "    def evaluateWordWildcard(self, argument):\n",
    "        return self.GetWordWildcard(argument[0])\n",
    "        \n",
    "    def evaluate(self, argument):\n",
    "        return self._methods[argument.getName()](argument)\n",
    "\n",
    "    def Parse(self, query):\n",
    "        #print self._parser(query)[0]\n",
    "        return self.evaluate(self._parser(query)[0])\n",
    "\n",
    "    def GetWord(self, word):\n",
    "        return query(word)\n",
    "\n",
    "    def GetWordWildcard(self, word):\n",
    "        return query(word)\n",
    "\n",
    "    def GetQuotes(self, search_string, tmp_result):\n",
    "        return query(search_string, mode=\"and\")\n",
    "\n",
    "    def GetNot(self, not_set):\n",
    "        return Set().difference(not_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "204184b1-84df-45ed-ab85-d89af73f01d1"
    }
   },
   "source": [
    "# Indexer and querying system\n",
    "\n",
    "This notebook provides a fonctionnal system to index a bunch of text document, and to run fast text queries against the index.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T15:38:47.743810",
     "start_time": "2016-11-02T15:38:47.737796"
    },
    "collapsed": false,
    "nbpresent": {
     "id": "f983e209-828d-4237-beac-fd05f1439232"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk.tokenize import *\n",
    "\n",
    "from os import listdir, mkdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import mmap\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "import math\n",
    "import operator\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "\n",
    "# from joblib import Parallel, delayed # Needed only if you use parallel in parsefile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T15:38:49.733179",
     "start_time": "2016-11-02T15:38:49.720181"
    },
    "collapsed": false,
    "nbpresent": {
     "id": "f9d1e156-1248-47ae-9b48-5a2e742b57b2"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() # Stemmer used to simplify words \n",
    "cachedStopWords = stopwords.words(\"english\") # Used to filter out useless words like this, the, a, ...\n",
    "docs = listdir(\"latimes_cleaned\")\n",
    "stemdb = {} # \n",
    "index = {}\n",
    "indexed = {}\n",
    "termcounts = {}\n",
    "titles = {}\n",
    "artcount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T15:38:50.060191",
     "start_time": "2016-11-02T15:38:50.055176"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "5673b0d2-fd2f-495a-b8a7-9dc0781912f0"
    }
   },
   "outputs": [],
   "source": [
    "# Save and load arbitrary things in python\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('data_cache/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('data_cache/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T15:43:22.704622",
     "start_time": "2016-11-02T15:43:22.668607"
    },
    "collapsed": false,
    "nbpresent": {
     "id": "ddeb4682-6fe5-44d7-ba78-8e602ead9385"
    }
   },
   "outputs": [],
   "source": [
    "def mergeindex(globalindex, localindex, docid): # Merge two index structure. not for merging PLs\n",
    "    for key in localindex.keys():\n",
    "        globalindex.setdefault(key,[]).append((docid, localindex[key]))\n",
    "\n",
    "def updatecount(globalcounts, localcounts): # Update references count\n",
    "    for key in localcounts.keys():\n",
    "        globalcounts[key] = globalcounts.setdefault(key, 0) + 1\n",
    "\n",
    "\n",
    "def parsestring(words): # Return a small index for a string\n",
    "    local_index = {}\n",
    "    for word in words:\n",
    "        word = re.sub(r'[^a-zA-Z0-9]+','', word)\n",
    "        if not word == \"\" and not word in cachedStopWords:\n",
    "            try:\n",
    "                stemmed = stemdb[word]\n",
    "            except:\n",
    "                stemmed = stemmer.stem(word)\n",
    "                stemdb[word] = stemmed\n",
    "            local_index[stemmed] = local_index.setdefault(stemmed, 0)+1\n",
    "    return local_index\n",
    "\n",
    "def parseArticle(art): # update the global index with the data from an article\n",
    "    global artcount\n",
    "    artcount += 1\n",
    "    words = []\n",
    "    if \"headline\" in art.keys():\n",
    "        words = words + art[\"headline\"].split() * 3\n",
    "        titles[art[\"docno\"]] = art[\"headline\"]\n",
    "    else : \n",
    "        titles[art[\"docno\"]] = art[\"docno\"]\n",
    "    if \"text\" in art.keys():\n",
    "        words = words + art[\"text\"].split()\n",
    "    lindex = parsestring(words)\n",
    "    mergeindex(index,lindex, art[\"docno\"])\n",
    "    updatecount(termcounts, lindex)\n",
    "\n",
    "def parsefile(document): # Run parseArticle for each article in a file\n",
    "    global artcount\n",
    "    local_index = {}\n",
    "    j = json.load(open(document))\n",
    "    \n",
    "    #Parallel(n_jobs=128, backend=\"threading\")(delayed(parseArticle)(art) for art in j)\n",
    "    for art in j:\n",
    "        parseArticle(art)\n",
    "\n",
    "        \n",
    "def addToIndex(doc, index, indexed, termcount): # Ensure that a file is indexed only once. \n",
    "    if not doc in indexed.keys() :\n",
    "        local_index = parsefile(\"latimes_cleaned/\"+doc)\n",
    "        indexed[doc] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T15:43:23.509040",
     "start_time": "2016-11-02T15:43:23.502041"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'amet': 1,\n",
       " 'dolor': 1,\n",
       " 'impsum': 1,\n",
       " 'lorem': 1,\n",
       " 'sit': 1,\n",
       " 'string': 1,\n",
       " 'test': 1}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time parsestring('lorem impsum dolor sit amet this is a test string !%ùàéèêë'.split()) # unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction with the indexer\n",
    "\n",
    "The block bellow will erase all the index and rebuild it from the files in the dataset. \n",
    "On my computer, it takes about 15 minutes to go through all the documents, so have a coffee machine handy if you plan to run this on a phone (it takes about 3 hours).\n",
    "\n",
    "I totally recommend you to use the loading system instead, see bellow.\n",
    "\n",
    "If, for some reason, you need to modify the index, here are a few tips :\n",
    "* If you just need to add an entry, you can do so without rebuilding the whole thing. just load the index and use addToIndex\n",
    "* If you are testing, use python's slice system to load only a few documents : replace docs with docs[:30] to load only the first 30 documents\n",
    "* If you need to build the whole index, grab a coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T15:51:38.113430",
     "start_time": "2016-11-02T15:43:37.919673"
    },
    "collapsed": false,
    "nbpresent": {
     "id": "0407a813-c195-4f0c-bfd6-cbccb71fcbbb"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 730/730 [06:20<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "# This bloc recreate the whole index. Use load instead if possible\n",
    "\n",
    "def flushIndexToFile(fname): # Flush the global index to a partial posting list\n",
    "    try:\n",
    "        os.remove(\"partialPLS/\"+str(fname))\n",
    "    except:\n",
    "        pass\n",
    "    with open(\"partialPLS/\"+str(fname),'w+b') as f:\n",
    "        for key in sorted(index.keys()):\n",
    "            j = json.dumps(index[key])\n",
    "            toWrite = key+\"\\n\"+j+\"\\n\"\n",
    "            f.write(toWrite.encode('utf8'))\n",
    "        f.flush()\n",
    "        \n",
    "def mergePartialPLs(count): # Merge the PLs to a single, and big, file. You need to specify the number of partial PLs to merge\n",
    "    global ifindex\n",
    "    ifindex = {}\n",
    "    offset = 0\n",
    "    try:\n",
    "        mfid.close()\n",
    "        mfscore.close()\n",
    "    except:\n",
    "        pass # Python !\n",
    "    with open('PLByDocID', 'w+b') as f:\n",
    "        with open('PLByScore', 'w+b') as g:\n",
    "            cursors = []\n",
    "            for fname in range(0, count):\n",
    "                h =open(\"partialPLs/\"+str(fname),'r+b')\n",
    "                cursors.append([h.readline().decode('utf8').rstrip(), h])\n",
    "            while len(cursors) > 0:\n",
    "                # Simply put, this bloc will create a cursor for each posting list, and at each iteration, \n",
    "                # advance the lowest cursor(s) while writing the merged content of the elements that have been forgotten in the final posting list, \n",
    "                # and keeps track of the length of the content to save the offset of this word in the posting list\n",
    "                # We end up with two ordered file : they are ordered by word, then by docid or score, respectively\n",
    "                # We also know the offset for each word, so we can reach a specific word in the list in constant time.\n",
    "                cursors = sorted(cursors, key= operator.itemgetter(0))\n",
    "                tempIndex = []\n",
    "                tempRef = 0\n",
    "                tempWord = cursors[0][0]\n",
    "                while tempRef < len(cursors) and cursors[tempRef][0] == tempWord :\n",
    "                    toParse = cursors[tempRef][1].readline().decode('utf8').rstrip()\n",
    "                    try:\n",
    "                        lind = json.loads(toParse)\n",
    "                        for el in lind:\n",
    "                            tempIndex.append(el)\n",
    "                    except:\n",
    "                        print(\"Error raised for word \"+str(tempWord)+\" in file \"+str(tempRef))\n",
    "                    \n",
    "                    cursors[tempRef][0] = cursors[tempRef][1].readline().rstrip()\n",
    "                    if cursors[tempRef][0] == b'':\n",
    "                        cursors[tempRef][1].close()\n",
    "                        del cursors[tempRef]\n",
    "                        tempRef -= 1 # Else we would skip a file\n",
    "                    tempRef += 1\n",
    "                j1 = json.dumps(sorted(tempIndex, key = operator.itemgetter(0))).encode('utf8')\n",
    "                j2 = json.dumps(sorted(tempIndex, key = operator.itemgetter(1))[::-1]).encode('utf8')\n",
    "                f.write(j1)\n",
    "                g.write(j2)\n",
    "                ifindex[tempWord] = (offset, len(j1))\n",
    "                offset += len(j1)\n",
    "            g.flush()\n",
    "        f.flush()\n",
    "\n",
    "def buildIndex():\n",
    "    docs = listdir(\"latimes_cleaned/\")\n",
    "    # Well... \n",
    "    global index\n",
    "    global indexed\n",
    "    global termcounts\n",
    "    global titles\n",
    "    global artcount\n",
    "    index = {}\n",
    "    indexed = {}\n",
    "    termcounts = {}\n",
    "    titles = {}\n",
    "    artcount = 0\n",
    "    trig = False\n",
    "    partialPL = 0\n",
    "    try:\n",
    "        mkdir(\"partialPLs\")\n",
    "    except:\n",
    "        pass # Python !!!\n",
    "    for doc in tqdm(docs) : # I totally love tqdm. If you don't want to install this, just replace tqdm(docs) with docs. You won't have the awesome progress bar though\n",
    "        addToIndex(doc, index, indexed, termcounts)\n",
    "\n",
    "        if sys.getsizeof(index) > 1200000: # Max size of the index, will flush if index becomes bigger than this\n",
    "            flushIndexToFile(partialPL)\n",
    "            partialPL += 1\n",
    "            index = {}\n",
    "    if len(index)>0:        \n",
    "        flushIndexToFile(partialPL)\n",
    "        partialPL +=1\n",
    "    mergePartialPLs(partialPL)   \n",
    "    \n",
    "def save():\n",
    "    all_index = (index, indexed, termcounts, titles, artcount, ifindex)\n",
    "    save_obj(all_index, \"indexFile_cleaneddata\")\n",
    "\n",
    "%prun -s \"cumulative\" buildIndex()\n",
    "save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block bellow saves the whole index as a pickle file.\n",
    "This way, you can load the index afterwards, and save a precious time !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T15:54:03.057720",
     "start_time": "2016-11-02T15:54:02.525054"
    },
    "collapsed": false,
    "nbpresent": {
     "id": "b32885ed-3fa8-43c2-9895-a9c9a6015f5c"
    }
   },
   "outputs": [],
   "source": [
    "def save():\n",
    "    all_index = (index, indexed, termcounts, titles, artcount, ifindex)\n",
    "    save_obj(all_index, \"indexFile_cleaneddata\")\n",
    "save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the block bellow loads all the index from disk. about half a second for the whole thing on my computer.\n",
    "Faster than the ~6 minutes required to build the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T15:54:06.580350",
     "start_time": "2016-11-02T15:54:06.160352"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully imported 730 indexed documents\n"
     ]
    }
   ],
   "source": [
    "all_index = load_obj(\"indexFile_cleaneddata\")\n",
    "index, indexed, termcounts, titles, artcount, ifindex = all_index\n",
    "print(\"successfully imported {count} indexed documents\".format(count=len(indexed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query system\n",
    "\n",
    "Bellow is the querying system. Use it to, well, query the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T16:46:06.907875",
     "start_time": "2016-11-02T16:46:06.864866"
    },
    "collapsed": false,
    "nbpresent": {
     "id": "c1542422-4a08-44c3-acdf-1a3216820bfb"
    }
   },
   "outputs": [],
   "source": [
    "with open('PLByDocID', 'r+b') as f:\n",
    "    mfid = mmap.mmap(f.fileno(),0)\n",
    "with open('PLByScore', 'r+b') as g:\n",
    "    mfscore = mmap.mmap(g.fileno(),0)\n",
    "\n",
    "def getWordPL(word):\n",
    "    global ifindex\n",
    "    try:\n",
    "        offset, length = ifindex[word]\n",
    "        mfid.seek(offset)\n",
    "        j = mfid.read(length)\n",
    "        rbind = json.loads(j.decode())\n",
    "        return rbind\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def query(request, mode=\"or\"):\n",
    "    words = request.lower().split()\n",
    "    scores = {}\n",
    "    matches = []\n",
    "    global artcount\n",
    "    for word in words:\n",
    "        stemmed = stemmer.stem(word)\n",
    "        \n",
    "        ids = []\n",
    "        if stemmed in termcounts.keys():\n",
    "            localresults = getWordPL(stemmed.encode())\n",
    "            idf = math.log(artcount/termcounts[stemmed])\n",
    "\n",
    "            for match in localresults:\n",
    "                docid, score = match\n",
    "                ids.append(docid)\n",
    "                score = score * idf\n",
    "                scores[docid] = scores.setdefault(docid, 0) + score\n",
    "        matches.append(ids)\n",
    "    if mode == \"and\":\n",
    "        found = set(matches[0]).intersection(*matches)\n",
    "        notfound = list(set(scores)-set(found))\n",
    "        for miss in notfound:\n",
    "            del scores[miss]\n",
    "    return scores\n",
    "\n",
    "def getBestSimple(request, nbresults=10, mode=\"or\"):\n",
    "    scores = query(request, mode)\n",
    "    return sorted(scores.items(), key=operator.itemgetter(1))[:-nbresults:-1]\n",
    "\n",
    "def translateTitles(response):\n",
    "    modified = []\n",
    "    for pair in response:\n",
    "        docno, score = pair\n",
    "        modified.append((titles[docno], score))\n",
    "    return modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple queries\n",
    "\n",
    "This is where you enter the queries for now. This system supports only simple queries, composed by a list of words and a mode of selection (or or and).\n",
    "\n",
    "There is a much better query system bellow. with a lexer, a parser, parenthesis, and a few other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T16:46:09.852071",
     "start_time": "2016-11-02T16:46:09.847075"
    },
    "collapsed": false,
    "nbpresent": {
     "id": "6396603b-b6c9-4ba6-9a7a-0f114f493501"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('la110290-0195', 54.564590567172104),\n",
       " ('la092090-0257', 43.65167245373768),\n",
       " ('la090789-0114', 38.195213397020474),\n",
       " ('la120690-0033', 32.73875434030326),\n",
       " ('la090689-0017', 32.73875434030326),\n",
       " ('la121990-0091', 27.282295283586052),\n",
       " ('la031789-0039', 27.282295283586052),\n",
       " ('la122990-0066', 27.282295283586052),\n",
       " ('la090490-0044', 21.82583622686884)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getBestSimple(\"towel\", mode=\"and\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex queries\n",
    "\n",
    "This other system is more advanced, and features a lexer and a parser to support complex queries with various operator and parenthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T16:46:11.645829",
     "start_time": "2016-11-02T16:46:11.640826"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parserTest = SearchQueryParser()\n",
    "def getBest(request, nbresults=10):\n",
    "    scores = parserTest.Parse(request)\n",
    "    return sorted(scores.items(), key=operator.itemgetter(1))[:-nbresults:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T16:46:12.409676",
     "start_time": "2016-11-02T16:46:12.400664"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"southern california's environment at the crossroads  waste: the disappearing spaces for tons of trash  \",\n",
       "  123.40572101108432),\n",
       " ('w. hollywood moves to end trash-hauling confusion  ', 64.19376237612065),\n",
       " ('fixer-upper art  why many contemporary artworks are falling apart -- and how conservators are  fighting back  ',\n",
       "  44.788759815502765),\n",
       " ('your collectibles: beer not necessarily a liquid asset  ',\n",
       "  44.74128596059276),\n",
       " ('private n.y. garbage collectors strike  ', 44.59886439586276),\n",
       " ('garbage art gets a new twist in the hands of muscovite ilya kabakov  ',\n",
       "  44.456442831132755),\n",
       " ('wide-ranging controls will be costly  clean air: businesses not breathing easily  ',\n",
       "  24.766597141054866),\n",
       " (\"southern california's environment at the crossroads  the coastline: debating development along the shores  public interest in the coast, which ebbs and flows, is again on the rise.  assault on nature at the edge of the ocean  \",\n",
       "  24.719123286144864),\n",
       " ('the pack-rat syndrome  some people live and die in garbage-choked houses, and no one understands why  ',\n",
       "  24.719123286144864)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translateTitles(getBest('garbage collector', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T16:46:22.438306",
     "start_time": "2016-11-02T16:46:22.431309"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('la110290-0195', 54.564590567172104),\n",
       " ('la092090-0257', 43.65167245373768),\n",
       " ('la090789-0114', 38.195213397020474),\n",
       " ('la120690-0033', 32.73875434030326),\n",
       " ('la090689-0017', 32.73875434030326),\n",
       " ('la121990-0091', 27.282295283586052),\n",
       " ('la031789-0039', 27.282295283586052),\n",
       " ('la122990-0066', 27.282295283586052),\n",
       " ('la090490-0044', 21.82583622686884)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getBest('towel', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T16:46:28.462497",
     "start_time": "2016-11-02T16:46:28.415497"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from http.server import BaseHTTPRequestHandler,HTTPServer\n",
    "from socketserver import ThreadingMixIn\n",
    "import threading\n",
    "from urllib.parse import urlparse\n",
    "from urllib import parse\n",
    "import argparse\n",
    "import cgi\n",
    "\n",
    "try:\n",
    "    serv.stop()\n",
    "except:\n",
    "    pass\n",
    "class LocalData(object):\n",
    "  records = {}\n",
    " \n",
    "class HTTPRequestHandler(BaseHTTPRequestHandler):\n",
    " \n",
    "  def do_GET(self):\n",
    "    self.send_response(200)\n",
    "    self.send_header('Content-Type', 'text/html')\n",
    "    self.end_headers()\n",
    "    print(urlparse(self.path))\n",
    "    query = urlparse(self.path)\n",
    "    if query.path == \"/\":\n",
    "        ret = '<html><body>It works</body></html>'\n",
    "    elif query.path == \"/query\":\n",
    "        rq = parse.unquote(query.query)\n",
    "        outp = translateTitles(getBest(rq))\n",
    "        ret =  '<table style=\"width:100%\"><tr><th>Article</th><th>Score</th></tr>'\n",
    "        for elm in outp:\n",
    "            ret = ret+'<tr><th>%s</th><th>%s</th></tr>'%(elm[0], elm[1])\n",
    "        ret = ret+\"</table>\"\n",
    "    self.wfile.write(ret.encode('utf8'))\n",
    "    '''outp = translateTitles(getBest('towel'))\n",
    "    ret =  '<html><body><table style=\"width:100%\"><tr><th>Article</th><th>Score</th></tr>'\n",
    "    for elm in outp:\n",
    "        ret = ret+'<tr><th>%s</th><th>%s</th></tr>'%(elm[0], elm[1])\n",
    "    ret = ret+\"</table></body></html>\"\n",
    "    self.wfile.write(ret.encode('utf8'))'''\n",
    "    \n",
    "    return\n",
    " \n",
    "class ThreadedHTTPServer(ThreadingMixIn, HTTPServer):\n",
    "  allow_reuse_address = True\n",
    " \n",
    "  def shutdown(self):\n",
    "    self.socket.close()\n",
    "    HTTPServer.shutdown(self)\n",
    " \n",
    "class SimpleHttpServer():\n",
    "  def __init__(self, ip, port):\n",
    "    self.server = ThreadedHTTPServer((ip,port), HTTPRequestHandler)\n",
    " \n",
    "  def start(self):\n",
    "    self.server_thread = threading.Thread(target=self.server.serve_forever)\n",
    "    self.server_thread.daemon = True\n",
    "    self.server_thread.start()\n",
    " \n",
    "  def waitForThread(self):\n",
    "    self.server_thread.join()\n",
    " \n",
    "  def addRecord(self, recordID, jsonEncodedRecord):\n",
    "    LocalData.records[recordID] = jsonEncodedRecord\n",
    " \n",
    "  def stop(self):\n",
    "    self.server.shutdown()\n",
    "    self.waitForThread()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T16:46:28.882909",
     "start_time": "2016-11-02T16:46:28.877905"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Nov/2016 16:46:32] \"GET /query?crash HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParseResult(scheme='', netloc='', path='/query', params='', query='crash', fragment='')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Nov/2016 16:46:39] \"GET /query?nuclear%20bomb HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParseResult(scheme='', netloc='', path='/query', params='', query='nuclear%20bomb', fragment='')\n"
     ]
    }
   ],
   "source": [
    "serv = SimpleHttpServer(\"127.0.0.1\", 5366)\n",
    "serv.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-02T15:09:02.266828",
     "start_time": "2016-11-02T15:09:02.263814"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "serv.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Index and query - Reformatted data.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "d76c8e06-3f71-4ecd-b868-87f9254b0555",
    "theme": {
     "d3d7867b-1ee4-4a7e-836f-07ba964a2c48": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "d3d7867b-1ee4-4a7e-836f-07ba964a2c48",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         34,
         34,
         34
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         238,
         238,
         238
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         170,
         34,
         51
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         238,
         238,
         238
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Ubuntu",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Ubuntu",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Ubuntu",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Ubuntu",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Ubuntu"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Ubuntu"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Ubuntu"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Ubuntu",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Ubuntu",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Ubuntu",
       "font-size": 5
      }
     },
     "d76c8e06-3f71-4ecd-b868-87f9254b0555": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "d76c8e06-3f71-4ecd-b868-87f9254b0555",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         256,
         256,
         256
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         0,
         0,
         0
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         0,
         0,
         139
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         0,
         0,
         0
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "News Cycle",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "News Cycle"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "News Cycle"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "News Cycle"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     },
     "ff64e434-5bb4-459e-a611-f167c1d10393": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "ff64e434-5bb4-459e-a611-f167c1d10393",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         34,
         34,
         34
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         238,
         238,
         238
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         170,
         34,
         51
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         238,
         238,
         238
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Ubuntu",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Ubuntu",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Ubuntu",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Ubuntu",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Ubuntu"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Ubuntu"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Ubuntu"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Ubuntu",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Ubuntu",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Ubuntu",
       "font-size": 5
      }
     }
    }
   }
  },
  "notify_time": "5",
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
